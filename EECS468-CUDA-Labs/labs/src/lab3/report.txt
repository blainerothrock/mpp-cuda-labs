Authors: Grant Gasser (GGL8333), Nayan Mehta (NMH3011), Blaine Rothrock (BRY4668)

——————————————
Instructions (DELETE LATER), need to adjust to fit this format

In addition to the code, add a report file in your lab3 source directory. Your report
should contain the names and netIDs of all team members, and a journal of all
optimizations you tried, including those that were ultimately abandoned, or worsened
the performance, or did not work. Your report should have an entry for every
optimization tried, and each entry should briefly note:
a. What is the goal for the optimization, and briefly describe the changes you made in
the source files for that optimization.
b. Any difficulties with completing the optimization correctly.
c. The man-hours spent developing the optimization (even if it was abandoned or not
working). This will be an indication of the optimization’s difficulty.
d. If finished and working, the speedup of the code after the optimization was applied.
Please be brief in the descriptions above; there is no need for lengthy descriptions. Even
bulleted lists are enough if they convey everything you want to say.
——————————————

Strategy 1: Block partitioning: Divide input into sections. Each thread processes a section. Use atomic add to avoid race conditions.

Drawback: “large number of simultaneously active threads in an SM typically cause too much interference in the caches that one cannot expect a data in a cache line to remain available for all the sequential accesses by a thread”

Strategy 2 (several x faster): Interleaved partitioning: to achieve memory coalescing. All memory elements fetched from DRAM at one time (one coalesced mem access). Each thread will jump blockDim.x * gridDim.x (# of threads) to process elements.

“each thread will process elements that are separated by the elements processed by all threads during one iteration”


Latency vs throughput of atomic ops

Atomic ops => serialization => slows down prog

Duration of each atomic op = latency of mem read + latency of mem write = hundreds of clock cycles


Atomic ops in cache memory

Approach: reduce access latency to the heavily contended locations => use cache memories

Recent GPUs (not sure if gtx 680) can perform atomic ops in the last level cache (which is 10s of cycles instead of 100s) 

“Since the access time to the last level cache is in tens of cycles rather than hundreds of cycles, the throughput of atomic operations is improved by at least an order of magnitude by just allowing them to be performed in the last level cache”


Privatization

Place data in shared mem (private to each SM) 
	⁃	few cycles

Problem: The problem is that due to the private nature of shared memory, the updates by threads in one thread block are no longer visible to threads in other blocks. The programmer must explicitly deal with this lack of visibility of histogram updates across thread blocks.

Privatization: The idea is to replicate highly contended output data structures into private copies so that each thread (or each subset of threads) can update its private copy. Private copies accessed much faster and w less contention

Drawback: need to merge these private sub histograms

1 hist per block

“Under this scheme, a few hundred threads would work on a copy of the histogram stored in short-latency shared memory, as opposed to tens of thousands of threads pounding on a histogram stored in medium latency second level cache or long latency DRAM.”
—–—

This is prob good enough for our project. Time permitting, we can try Aggregation. Should add timing code to measure each method we try.


Aggregation

Some data sets have a large concentration of identical data values in localized areas
	⁃	so aggregation probably good for images

“each thread to aggregate consecutive updates into a single update if they are updating the same element of the histogram” 


Should we do thread checking? i < n
